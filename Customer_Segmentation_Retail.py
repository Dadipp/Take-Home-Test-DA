# -*- coding: utf-8 -*-
"""Take Home Test DA.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1SeCTHLFBvTcjplgYXMSdaN7O_6_OSBX2

# Problem Statement

"Bagaimana distribusi dan tren perilaku pelanggan berdasarkan segmentasi RFM, dan segmen mana yang memberikan kontribusi terbesar terhadap pendapatan perusahaan?"

"Bagaimana tren penjualan dan retensi pelanggan dari waktu ke waktu, serta strategi apa yang dapat diterapkan untuk meningkatkan loyalitas dan mengurangi Lost Customer?"

# Data Understanding
"""

# ===============================
# 1. Import Library
# ===============================
import pandas as pd
import numpy as np

# ===============================
# 2. Load Dataset
# ===============================
# Pastikan kamu sudah upload file CSV ke Google Colab atau mount Google Drive
clientes = pd.read_csv('Clientes.csv')
vendas = pd.read_csv('Vendas.csv')
produtos = pd.read_csv('Produtos.csv')

# ===============================
# 3. Cek Data Awal
# ===============================
print("Clientes:", clientes.shape)
print("Vendas:", vendas.shape)
print("Produtos:", produtos.shape)

clientes.info()

vendas.info()

produtos.info()

clientes.rename(columns={
    "Cliente_ID": "Customer_ID",
    "Nome": "Customer_Name",
    "Idade": "Age",
    "Género": "Gender",
    "Cidade": "City",
    "Canal de Compra": "Purchase_Channel",
    "Total de Compras": "Total_Purchases",
    "Imagem": "Image_URL"
}, inplace=True)

vendas.rename(columns={
    "Venda_ID": "Sale_ID",
    "Loja_ID": "Store_ID",
    "Produto_ID": "Product_ID",
    "Cliente_ID": "Customer_ID",
    "Colaborador_ID": "Staff_ID",
    "Quantidade": "Quantity",
    "Preço Unitário": "Unit_Price",
    "Data da Venda": "Sale_Date",
    "Canal de Venda": "Sales_Channel"
}, inplace=True)

# Convert Sale_Date ke datetime
vendas["Sale_Date"] = pd.to_datetime(vendas["Sale_Date"], format="%Y-%m-%d %H:%M:%S")

produtos.rename(columns={
    "Produto_ID": "Product_ID",
    "Nome": "Product_Name",
    "Categoria": "Category",
    "Cor": "Color",
    "Descrição": "Description",
    "Tamanho": "Size",
    "Preço": "Price",
    "Custo_Aquisição": "Acquisition_Cost",
    "Imagem": "Image_URL"
}, inplace=True)

# ===============================
# 5. Merge Dataset
# ===============================

# Merge sales + customers
sales_customers = vendas.merge(clientes, on="Customer_ID", how="left")

# Merge dengan products
merged_data = sales_customers.merge(produtos, on="Product_ID", how="left")

# ===============================
# 6. Output hasil
# ===============================
print("Merged Data Shape:", merged_data.shape)
merged_data.head()

"""# Data Preprocessing"""

# Cek Missing Value
merged_data.isnull().sum()

# Cek Duplikat pada Data
merged_data.duplicated().sum()

merged_data.info()

"""# Feature Engineering"""

from datetime import timedelta

# Pastikan kolom Sale_Date dalam format datetime
merged_data["Sale_Date"] = pd.to_datetime(merged_data["Sale_Date"])

# Tentukan tanggal referensi (misal, sehari setelah transaksi terakhir)
reference_date = merged_data["Sale_Date"].max() + timedelta(days=1)

# Hitung Recency (hari sejak transaksi terakhir)
rfm_recency = merged_data.groupby("Customer_ID")["Sale_Date"].max().apply(lambda x: (reference_date - x).days)

# Hitung Frequency (jumlah transaksi)
rfm_frequency = merged_data.groupby("Customer_ID")["Sale_ID"].nunique()

# Hitung Monetary (total nilai pembelian)
rfm_monetary = (merged_data["Unit_Price"] * merged_data["Quantity"]).groupby(merged_data["Customer_ID"]).sum()

# Gabungkan ke DataFrame RFM
rfm_df = pd.DataFrame({
    "Recency": rfm_recency,
    "Frequency": rfm_frequency,
    "Monetary": rfm_monetary
}).reset_index()

# Merge kembali ke merged_data
merged_data = merged_data.merge(rfm_df, on="Customer_ID", how="left")

"""# Cek Outlier dari Data"""

# cek outlier dari data
import seaborn as sns

# Function to detect outliers using IQR
def detect_outliers_iqr(df, column):
    q1 = merged_data[column].quantile(0.25)
    q3 = merged_data[column].quantile(0.75)
    iqr = q3 - q1
    lower_bound = q1 - 1.5 * iqr
    upper_bound = q3 + 1.5 * iqr
    outliers = merged_data[(merged_data[column] < lower_bound) | (merged_data[column] > upper_bound)]
    return outliers

# Example usage for numerical columns
numerical_cols = merged_data.select_dtypes(include=np.number).columns

import matplotlib.pyplot as plt
for col in numerical_cols:
    plt.figure(figsize=(8, 4))
    sns.boxplot(x=merged_data[col])
    plt.title(f'Box plot of {col}')
    plt.show()

# d. Top Products by Quantity and Sales Value
top_products_quantity = merged_data.groupby("Product_Name")["Quantity"].sum().sort_values(ascending=False).head(5)
top_products_sales = (merged_data["Unit_Price"] * merged_data["Quantity"]).groupby(merged_data["Product_Name"]).sum().sort_values(ascending=False).head(5)
print("\nTop 5 Products by Quantity:\n", top_products_quantity)
print("\nTop 5 Products by Sales Value:\n", top_products_sales)

# e. Monthly Sales and Customer Trends
merged_data["YearMonth"] = merged_data["Sale_Date"].dt.to_period("M")
monthly_sales = (merged_data["Unit_Price"] * merged_data["Quantity"]).groupby(merged_data["YearMonth"]).sum()
monthly_customers = merged_data.groupby("YearMonth")["Customer_ID"].nunique()
print("\nMonthly Sales Trend:\n", monthly_sales)
print("\nMonthly Unique Customers Trend:\n", monthly_customers)

# Daftar nama produk yang dianggap anomali
anomali_produk = ["Error", "Test", "Unknown", "N/A", ""]

# Simpan jumlah data sebelum pembersihan
before_clean = len(merged_data)

# Filter: hapus baris dengan Product_Name anomali atau nilai kosong
merged_data = merged_data[~merged_data["Product_Name"].isin(anomali_produk)]

# Hapus baris dengan Product_Name null
merged_data = merged_data[merged_data["Product_Name"].notnull()]

# Simpan jumlah data setelah pembersihan
after_clean = len(merged_data)

print(f"Data sebelum pembersihan: {before_clean}")
print(f"Data setelah pembersihan: {after_clean}")
print(f"Jumlah data yang dihapus: {before_clean - after_clean}")

# Cek tipe fitur
cat_cols = merged_data.select_dtypes('object').columns.tolist()
num_cols = merged_data.select_dtypes(include=['int64', 'float64']).columns.tolist()
print("Fitur kategorikal:", cat_cols)
print("Fitur numerik:", num_cols)

"""# EDA"""

# Distribusi data numerik dalam grid layout
import math

# Ambil kolom numerik, lalu drop kolom ID atau kode
exclude_cols = ['Sale_ID', 'Product_ID', 'Staff_ID', 'Customer_ID', 'Campaign_ID','Store_ID', 'Sales_Channel', 'Gender', 'Purchase_Channel', 'Category', 'Sales_Generated', 'Investment']  # tambahkan ID lain jika perlu
num_cols = [col for col in merged_data.select_dtypes(include=[np.number]).columns if col not in exclude_cols]

# Tentukan grid
n_cols = 3
n_rows = math.ceil(len(num_cols) / n_cols)

fig, axes = plt.subplots(n_rows, n_cols, figsize=(n_cols * 4, n_rows * 3))
axes = axes.flatten()

for i, col in enumerate(num_cols):
    sns.histplot(merged_data[col], kde=True, bins=30, ax=axes[i])
    axes[i].set_title(f'Distribusi {col}', fontsize=10)

# Hapus subplot kosong
for j in range(i + 1, len(axes)):
    fig.delaxes(axes[j])

plt.tight_layout()
plt.show()

"""## Insight Distribusi Data

### 1. Quantity
- Distribusi relatif merata di rentang 0–100 unit per transaksi.  
- Tidak ada konsentrasi besar di titik tertentu, menandakan variasi jumlah barang per transaksi cukup seimbang.

### 2. Unit Price
- Rentang harga unit bervariasi lebar (0–1000).  
- Distribusi terlihat relatif merata tanpa puncak dominan, artinya tidak ada rentang harga tertentu yang sangat mendominasi.

### 3. Age
- Pelanggan tersebar di usia 20–70 tahun.  
- Tidak ada puncak tunggal yang mencolok, meski jumlah pelanggan usia 25–35 dan 55–65 sedikit lebih tinggi.

### 4. Total Purchases
- Terdistribusi di rentang 100–1000, cukup merata.  
- Tidak ada pelanggan tunggal yang mendominasi total pembelian.

### 5. Price
- Rentang harga transaksi berada di 20–100, distribusi cenderung merata.  
- Tidak ada segmen harga final yang benar-benar dominan.

### 6. Acquisition Cost
- Biaya akuisisi tersebar di kisaran 10–50, dengan sedikit puncak di sekitar 15–20 dan 35–45.  
- Artinya strategi akuisisi bervariasi dan tidak hanya di satu level biaya.

### 7. Recency
- Distribusi sangat _right-skewed_: mayoritas pelanggan memiliki **Recency rendah** (baru bertransaksi).  
- Sebagian kecil pelanggan memiliki Recency tinggi (lama tidak bertransaksi) → berpotensi churn.

### 8. Frequency
- Distribusi menyerupai **kurva normal** dengan mayoritas pelanggan bertransaksi sekitar **18–24 kali**.  
- Sedikit pelanggan yang memiliki frekuensi sangat rendah atau sangat tinggi.

### 9. Monetary
- Distribusi mendekati normal dengan puncak di kisaran **400k–600k**.  
- Tidak terlalu _right-skewed_, artinya sebagian besar pelanggan memiliki total belanja di rentang menengah, bukan ekstrem.
"""

top_products_quantity = (
    merged_data.groupby("Product_Name")["Quantity"]
    .sum()
    .sort_values(ascending=False)
    .head(5)
)

# 3️⃣ Plot grafiknya
plt.figure(figsize=(10, 6))
top_products_quantity.plot(kind='bar')
plt.title("Top 5 Products by Quantity Sold")
plt.xlabel("Product Name")
plt.ylabel("Quantity Sold")
plt.xticks(rotation=45)
plt.tight_layout()
plt.show()

"""## Insight: Top 5 Products by Quantity Sold

- **Modi** menempati posisi pertama dengan selisih cukup signifikan dibanding produk lain, sehingga layak menjadi fokus utama untuk promosi, persediaan, atau kampanye loyalitas pelanggan.  
- **Hic** dan **Adipisci** berada di posisi menengah dengan penjualan yang cukup tinggi dan stabil, keduanya memiliki potensi.
- **Id** dan **Enim** memiliki penjualan lebih rendah dibanding tiga besar, sehingga dapat diprioritaskan untuk strategi peningkatan penjualan seperti diskon, paket bundling, atau promosi silang.
"""

# 2. Top 5 Products by Sales Value
plt.figure(figsize=(10, 6))
top_products_sales.plot(kind='bar')
plt.title("Top 5 Products by Sales Value")
plt.xlabel("Product Name")
plt.ylabel("Sales Value")
plt.xticks(rotation=45)
plt.tight_layout()
plt.show()

"""## Insight: Top 5 Products by Sales Value

- **Modi** mendominasi total nilai penjualan, menunjukkan kontribusi signifikan terhadap pendapatan dan layak untuk diprioritaskan dalam strategi pemasaran.   
- **Adipisci**, **Hic**, dan **Id** memiliki nilai penjualan yang berdekatan, menunjukkan persaingan ketat di segmen ini.  
- **Enim** berada di posisi kelima dengan selisih cukup jauh dari puncak, sehingga berpotensi ditingkatkan melalui kampanye promosi atau penawaran khusus.
"""

# 3. Top 5 Regional Sales Contribution
regional_sales = (merged_data["Unit_Price"] * merged_data["Quantity"]).groupby(merged_data["City"]).sum().sort_values(ascending=False).head(5)
plt.figure(figsize=(10, 6))
regional_sales.plot(kind='bar')
plt.title("Top 5 Regional Sales Contribution")
plt.xlabel("City")
plt.ylabel("Total Sales")
plt.xticks(rotation=45)
plt.tight_layout()
plt.show()

"""## Insight: Top 5 Regional Sales Contribution

- **Lagos** menjadi penyumbang penjualan terbesar, dengan gap signifikan dibanding wilayah lainnya.  
- **Funchal** dan **São Brás de Alportel** memiliki kontribusi hampir seimbang, menunjukkan potensi pengembangan pasar di kedua area ini.  
- **Santa Cruz da Graciosa** dan **Sines** berada di urutan bawah namun masih masuk lima besar, sehingga tetap layak dipertahankan dengan strategi promosi terarah.  
"""

# 4. Monthly Sales Trend
plt.figure(figsize=(12, 6))
monthly_sales.plot(title="Monthly Sales Trend (2014-2025)")
plt.xlabel("Year-Month")
plt.ylabel("Total Sales")
plt.xticks(rotation=45)
plt.tight_layout()
plt.show()

"""## Insight: Monthly Sales Trend (2014–2025)

- Data menunjukkan tren penjualan bulanan yang sangat fluktuatif sepanjang periode 2014 hingga 2025. Tidak ada tren naik yang konsisten seperti yang Anda sebutkan, melainkan serangkaian puncak dan lembah yang terjadi secara berulang setiap tahun.
- Puncak Terbesar: Puncak penjualan tertinggi terlihat jelas terjadi pada pertengahan tahun 2018 (sekitar kuartal kedua dan ketiga).
- Penurunan Signifikan: Penurunan tajam memang terjadi pada awal tahun 2020, yang secara historis bertepatan dengan dimulainya pandemi COVID-19. Ini adalah korelasi yang masuk akal.
- Puncak Lain: Terdapat juga puncak yang cukup tinggi pada pertengahan 2016 dan 2021.
- Karena pola fluktuasi yang berulang, perusahaan dapat mengidentifikasi pola musiman (seasonal) dalam penjualan. Sehingga dapat dilakukan:
  - Perencanaan Persediaan (Inventory Planning): Mengatur stok barang agar tidak kekurangan saat penjualan tinggi atau kelebihan saat penjualan rendah.
  - Strategi Pemasaran: Mengaktifkan kampanye pemasaran atau promosi menjelang periode puncak untuk memaksimalkan keuntungan.

Kesimpulan: Secara keseluruhan, grafik menunjukkan penjualan yang naik dan turun sepanjang waktu tanpa tren pertumbuhan yang jelas. Strategi bisnis harus fokus pada memanfaatkan pola musiman ini.
"""

# 5. Monthly Unique Customers Trend
plt.figure(figsize=(12, 6))
monthly_customers.plot(title="Monthly Unique Customers Trend (2014-2025)")
plt.xlabel("Year-Month")
plt.ylabel("Number of Unique Customers")
plt.xticks(rotation=45)
plt.tight_layout()
plt.show()

"""## Insight: Monthly Unique Customers Trend (2014–2025)

- Data menunjukkan jumlah pelanggan unik bulanan yang berfluktuasi secara signifikan sepanjang periode 2014 hingga 2025. Pola ini mengindikasikan adanya pengaruh musiman atau faktor-faktor eksternal yang berdampak pada jumlah pelanggan.

- Puncak Terbesar: Peningkatan jumlah pelanggan unik yang paling tajam dan tertinggi terjadi pada pertengahan tahun 2017 dan awal 2019, menunjukkan keberhasilan strategi akuisisi pelanggan pada periode tersebut.

- Penurunan Signifikan: Penurunan drastis terjadi pada awal tahun 2023, mencapai titik terendah dalam periode yang ditampilkan. Ini bisa menjadi indikasi adanya masalah eksternal, perubahan tren pasar, atau kegagalan strategi retensi pelanggan.

- Pola Musiman: Terlihat adanya pola berulang di mana jumlah pelanggan cenderung meningkat di pertengahan tahun dan menurun di awal tahun, meskipun tidak selalu konsisten.

- Karena pola fluktuasi yang berulang, perusahaan dapat mengidentifikasi pola musiman (seasonal) dalam perilaku pelanggan. Sehingga dapat dilakukan:

  - Strategi Pemasaran: Meluncurkan kampanye akuisisi pelanggan yang lebih intensif menjelang periode puncak dan strategi retensi pelanggan yang kuat pada periode penurunan.

  - Analisis Lebih Dalam: Melakukan analisis untuk memahami penyebab penurunan drastis di awal 2023, seperti perubahan kebijakan, kompetisi, atau perubahan preferensi pelanggan.

Kesimpulan: Secara keseluruhan, grafik menunjukkan jumlah pelanggan unik yang fluktuatif tanpa tren pertumbuhan yang jelas. Strategi bisnis harus fokus pada pemanfaatan pola musiman dan investigasi mendalam terhadap penyebab penurunan tajam untuk meningkatkan basis pelanggan secara berkelanjutan.
"""

monthly_sales.value_counts(normalize=True)

merged_data['Total_Purchases'].value_counts(normalize=True)

merged_data['Purchase_Channel'].value_counts(normalize=True)

merged_data['Category'].value_counts(normalize=True)

merged_data['Channel'].value_counts(normalize=True)

merged_data.head()

merged_data.info()

# Mapping Gender (asumsi 'Masculino' dan 'Feminino' atau terjemahannya)
gender_mapping = {
    'M': 0,
    'F': 1,
}
merged_data['Gender'] = merged_data['Gender'].map(gender_mapping)

# Mapping Sales_Channel
sales_channel_mapping = {
    'Online': 0,
    'Física': 1
}
merged_data['Sales_Channel'] = merged_data['Sales_Channel'].map(sales_channel_mapping)

# Mapping Purchase_Channel
purchase_channel_mapping = {
    'Online': 0,
    'Física': 1
}
merged_data['Purchase_Channel'] = merged_data['Purchase_Channel'].map(purchase_channel_mapping)

# Mapping Category
category_mapping = {
    'Calçado': 0,
    'Roupas': 1
}
merged_data['Category'] = merged_data['Category'].map(category_mapping)

# 2.3 Korelasi antar variabel numerik
plt.figure(figsize=(16,8))
sns.heatmap(merged_data[num_cols].corr(), annot=True, cmap='coolwarm', fmt=".2f")
plt.title("Heatmap Korelasi Variabel Numerik")
plt.show()

merged_data['Gender'].value_counts(normalize=True)

# Daftar kolom yang akan di-drop
cols_drop = [
    "Image_URL_x",
    "Image_URL_y"
]

# Drop kolom dari merged_data
merged_data.drop(columns=cols_drop, inplace=True)

# Daftar kolom yang akan di-drop
cols_drop = [
    "Sale_ID",
    "Product_ID",
    "Staff_ID",
    "Campaign_ID",
    "Channel",
    "Start_Date",
    "End_Date",
    "Customer_Name",
    "Image_URL_x",
    "Image_URL_y",
    "Description"
]

# Drop kolom dari merged_data
merged_data.drop(columns=cols_drop, inplace=True)

merged_data.shape

# Simpan merged_data ke file CSV
merged_data.to_csv("merged_data.csv", index=False)

print("Dataset merged_data berhasil disimpan sebagai merged_data.csv")

"""## Insight: Korelasi Variabel Numerik

- Store_ID memiliki korelasi positif cukup kuat dengan Campaign_ID (0.76) dan Investment (0.64), menunjukkan pola penempatan kampanye dan alokasi investasi cenderung mengikuti store tertentu.  
- Campaign_ID dan Investment memiliki korelasi sangat tinggi (0.92), sehingga salah satu bisa dipilih untuk menghindari redundansi.  
- Frequency dan Monetary menunjukkan korelasi positif cukup kuat (0.68), menandakan semakin sering transaksi, semakin besar nilai totalnya.  
- Recency berkorelasi negatif sedang dengan Store_ID (-0.60), mengindikasikan beberapa store memiliki pelanggan yang lebih sering kembali dalam waktu singkat.  
- Sebagian besar variabel lain memiliki korelasi rendah, menunjukkan bahwa masing-masing membawa informasi yang unik.  

"""

# Update num_cols to reflect the current columns in df
num_cols = [col for col in num_cols if col in merged_data.columns]

corr = merged_data[num_cols].corr()

plt.figure(figsize=(14, 8))
sns.heatmap(corr, annot=True, fmt='.2f', cmap='coolwarm', square=True)
plt.title("Correlation Heatmap")
plt.tight_layout()
plt.show()

# 1.4 Encoding categorical variables
from sklearn.preprocessing import LabelEncoder

le = LabelEncoder()
for col in cat_cols:
    merged_data[col] = le.fit_transform(merged_data[col])

# 1.5 Standarisasi data numerik
from sklearn.preprocessing import StandardScaler

scaler = StandardScaler()
merged_data[num_cols] = scaler.fit_transform(merged_data[num_cols])

print("Data setelah preprocessing:")
print(merged_data.head())

# ===============================
# 2. EDA
# ===============================

import matplotlib.pyplot as plt
import seaborn as sns

# 2.1 Analisis deskriptif
print("\nDeskripsi Data Numerik:")
print(merged_data[num_cols].describe())

from sklearn.preprocessing import LabelEncoder

# Pilih kolom kategorikal
categorical_cols = merged_data.select_dtypes(include=['object']).columns

# Buat salinan data untuk transformasi
encoded_data = merged_data.copy()

# Gunakan LabelEncoder untuk tiap kolom kategorikal
label_encoders = {}
for col in categorical_cols:
    le = LabelEncoder()
    encoded_data[col] = le.fit_transform(encoded_data[col])
    label_encoders[col] = le  # simpan encoder jika nanti dibutuhkan kembali

print("Encoding selesai. Contoh data:")
print(encoded_data.head())

# Pilih hanya kolom numerik
numeric_cols = merged_data.select_dtypes(include=[np.number]).columns

# Dictionary untuk menyimpan outlier
outliers_iqr = {}

for col in numeric_cols:
    Q1 = merged_data[col].quantile(0.25)
    Q3 = merged_data[col].quantile(0.75)
    IQR = Q3 - Q1
    lower_bound = Q1 - 1.5 * IQR
    upper_bound = Q3 + 1.5 * IQR

    outlier_rows = merged_data[(merged_data[col] < lower_bound) | (merged_data[col] > upper_bound)]

    if not outlier_rows.empty:
        outliers_iqr[col] = outlier_rows.shape[0]

print("Jumlah outlier per kolom (IQR):")
print(outliers_iqr)

from sklearn.preprocessing import StandardScaler

# Pilih kolom numerik
numeric_cols = encoded_data.select_dtypes(include=[np.number]).columns

# Standarisasi
scaler = StandardScaler()
encoded_data[numeric_cols] = scaler.fit_transform(encoded_data[numeric_cols])

print("Standarisasi selesai. Contoh data:")
print(encoded_data.head())

print("Statistik Deskriptif:")
print(merged_data.describe())

from scipy import stats

outliers_zscore = {}
for col in numeric_cols:
    z_scores = np.abs(stats.zscore(merged_data[col]))
    outlier_rows = merged_data[z_scores > 3]
    if not outlier_rows.empty:
        outliers_zscore[col] = outlier_rows.shape[0]

print("Jumlah outlier per kolom (Z-Score):")
print(outliers_zscore)

cols_drop = [
    "Sale_ID", "Product_ID", "Staff_ID", "Image_URL_x", "Image_URL_y",
    "Description", "Campaign_ID", "Campaign_Name", "Channel",
    "Start_Date", "End_Date"
]

data_segmentation = merged_data.drop(columns=cols_drop)

from datetime import datetime

# Tentukan tanggal referensi (misal: max tanggal penjualan di data)
reference_date = merged_data["Sale_Date"].max()

# Group by Customer
customer_features = merged_data.groupby("Customer_ID").agg({
    "Sale_Date": lambda x: (reference_date - x.max()).days,  # Recency
    "Sale_ID": "nunique",                                   # Frequency
    "Unit_Price": "sum",                                    # Monetary (total belanja)
    "Quantity": "mean",                                     # Average Basket Size
    "Price": "mean",                                        # Average Order Value
    "Sales_Channel": lambda x: x.mode()[0],                 # Preferred Sales Channel
    "Category": lambda x: x.mode()[0]                       # Preferred Category
}).reset_index()

# Rename kolom agar jelas
customer_features.rename(columns={
    "Sale_Date": "Recency_Days",
    "Sale_ID": "Frequency",
    "Unit_Price": "Monetary",
    "Quantity": "Avg_Basket_Size",
    "Price": "Avg_Order_Value",
    "Sales_Channel": "Preferred_Sales_Channel",
    "Category": "Preferred_Category"
}, inplace=True)

print(customer_features.head())

# Sales channel
plt.figure(figsize=(6,4))
sns.countplot(x="Preferred_Sales_Channel", data=customer_features)
plt.title("Preferred Sales Channel")

# Preferred category
plt.figure(figsize=(8,4))
sns.countplot(x="Preferred_Category", data=customer_features, order=customer_features["Preferred_Category"].value_counts().index)
plt.title("Preferred Category")
plt.xticks(rotation=45)
plt.show()

# =========================================
# 1. Import Library
# =========================================
import pandas as pd
import numpy as np
from sklearn.preprocessing import StandardScaler
from sklearn.cluster import KMeans
import matplotlib.pyplot as plt
import seaborn as sns

# =========================================
# 2. Pilih Fitur Numerik untuk Clustering
# =========================================
features = ["Recency_Days", "Frequency", "Monetary", "Avg_Basket_Size", "Avg_Order_Value"]
X = customer_features[features]

# =========================================
# 3. Scaling Data
# =========================================
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

# =========================================
# 4. Menentukan Jumlah Cluster Optimal (Elbow Method)
# =========================================
inertia = []
K_range = range(2, 8)  # coba dari 2 sampai 7 cluster

for k in K_range:
    kmeans = KMeans(n_clusters=k, random_state=42, n_init=10)
    kmeans.fit(X_scaled)
    inertia.append(kmeans.inertia_)

plt.figure(figsize=(6,4))
plt.plot(K_range, inertia, marker='o')
plt.title("Elbow Method for Optimal K")
plt.xlabel("Number of Clusters")
plt.ylabel("Inertia")
plt.show()

# =========================================
# 5. Jalankan K-Means dengan jumlah cluster pilihan
# =========================================
optimal_k = 4  # misalnya dari grafik elbow kita pilih 4
kmeans = KMeans(n_clusters=optimal_k, random_state=42, n_init=10)
customer_features["Cluster"] = kmeans.fit_predict(X_scaled)

# =========================================
# 6. Visualisasi Hasil Cluster (2D PCA untuk reduksi dimensi)
# =========================================
from sklearn.decomposition import PCA

pca = PCA(n_components=2)
pca_result = pca.fit_transform(X_scaled)
customer_features["PCA1"] = pca_result[:,0]
customer_features["PCA2"] = pca_result[:,1]

plt.figure(figsize=(8,6))
sns.scatterplot(data=customer_features, x="PCA1", y="PCA2", hue="Cluster", palette="Set2", s=80)
plt.title("Customer Segmentation with K-Means")
plt.show()

# =========================================
# 7. Ringkasan per Cluster
# =========================================
cluster_summary = customer_features.groupby("Cluster")[features].mean().round(2)
print(cluster_summary)

# Misalnya variabel dataframe final kamu bernama df_final
output_path = "customer_data.csv"

# Simpan ke CSV
customer_features.to_csv(output_path, index=False, encoding="utf-8")

print(f"Dataset berhasil disimpan di {output_path}")